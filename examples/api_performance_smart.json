{
  "title": "API Performance Crisis - Intelligent Diagnosis",
  "elements": [
    {
      "text": "Response times degraded from 200ms to 8s. I'll adapt my questions based on your answers to quickly isolate the root cause.",
      "id": "intro_text"
    },
    {
      "id": "timing",
      "options": [
        "Sudden - after our deployment 2 hours ago",
        "Gradual - degrading over days",
        "Intermittent - no clear pattern"
      ],
      "Sudden - after our deployment 2 hours ago": [
        {
          "text": "Focusing on recent changes. Let's identify what changed:",
          "id": "deployment_focus"
        },
        {
          "id": "deployed_changes",
          "options": [
            "New API endpoints",
            "Database schema changes",
            "Configuration updates",
            "Dependency updates"
          ],
          "New API endpoints": [
            {
              "id": "only_new_slow",
              "reveals": [
                {
                  "text": "Strong signal: Issue isolated to new code. Check for missing database indexes on queries used by these endpoints.",
                  "id": "isolated_issue"
                }
              ],
              "check": "Are ONLY the new endpoints slow?"
            }
          ],
          "Database schema changes": [
            {
              "id": "schema_change_type",
              "options": [
                "Added new indexes",
                "Modified existing columns",
                "Added foreign key constraints",
                "Dropped indexes"
              ],
              "select": "What type of schema change?"
            }
          ],
          "Configuration updates": [
            {
              "id": "config_details",
              "placeholder": "e.g., connection pool size, cache TTL",
              "input": "What specifically changed?"
            }
          ],
          "multi": "What was deployed? (select all that apply)"
        },
        {
          "text": "PATTERN DETECTED: Too many simultaneous changes. This violates deployment best practices.",
          "id": "too_many_changes",
          "when": "count(deployed_changes) >= 3"
        },
        {
          "id": "complex_strategy",
          "options": [
            "Rollback everything immediately",
            "Use feature flags to disable new code paths",
            "Analyze metrics to isolate the problematic change",
            "Accept the risk and debug in production"
          ],
          "when": "count(deployed_changes) >= 3",
          "select": "Given the high change complexity, what's your immediate strategy?"
        }
      ],
      "Gradual - degrading over days": [
        {
          "text": "This suggests organic growth or resource leak. Let me understand the pattern:",
          "id": "gradual_pattern"
        },
        {
          "id": "trending_metrics",
          "options": [
            "Request volume",
            "Database size",
            "Memory usage",
            "Active connections"
          ],
          "Request volume": [
            {
              "slider": "Traffic increase percentage",
              "id": "traffic_increase",
              "min": 0,
              "max": 500,
              "default": 50
            },
            {
              "id": "traffic_distribution",
              "options": [
                "Evenly distributed",
                "Concentrated on specific endpoints",
                "From specific geographic regions",
                "Suspicious patterns (possible attack)"
              ],
              "select": "Traffic distribution:"
            }
          ],
          "Database size": [
            {
              "id": "growing_tables",
              "rows": 2,
              "input": "Which tables are growing fastest?"
            }
          ],
          "Memory usage": [
            {
              "id": "oom_or_gc",
              "check": "Are you seeing OOM kills or GC pressure?"
            }
          ],
          "multi": "Which metrics are trending upward?"
        },
        {
          "text": "CORRELATION FOUND: Specific endpoints + database growth suggests those endpoints are creating excessive data or missing pagination.",
          "id": "correlation_found",
          "when": "selected(traffic_distribution, 'Concentrated on specific endpoints') && selected(trending_metrics, 'Database size')"
        },
        {
          "id": "verify_query",
          "placeholder": "SELECT table_name, pg_size_pretty(pg_total_relation_size(table_name)) FROM information_schema.tables WHERE table_schema = 'public' ORDER BY pg_total_relation_size(table_name) DESC LIMIT 10;",
          "rows": 3,
          "when": "selected(traffic_distribution, 'Concentrated on specific endpoints') && selected(trending_metrics, 'Database size')",
          "input": "Run this query to verify:"
        },
        {
          "text": "INSIGHT: Multiple growing metrics indicate cascade failure, not single root cause.",
          "id": "cascade_failure",
          "when": "count(trending_metrics) >= 3"
        },
        {
          "id": "cascade_trigger",
          "options": [
            "Slow queries causing connection pooling",
            "Memory pressure causing swap thrashing",
            "Cache misses causing database overload",
            "Retry storms amplifying the load"
          ],
          "when": "count(trending_metrics) >= 3",
          "select": "What likely triggered the cascade?"
        }
      ],
      "Intermittent - no clear pattern": [
        {
          "text": "Intermittent issues often have external triggers. Let's identify correlations:",
          "id": "intermittent_intro"
        },
        {
          "id": "incident_observations",
          "options": [
            "Specific time patterns",
            "After batch jobs run",
            "During data syncs",
            "When specific customers are active"
          ],
          "multi": "During incidents, what do you observe?"
        }
      ],
      "select": "When did this start?"
    },
    {
      "group": "Quick Verification",
      "id": "quick_verification",
      "elements": [
        {
          "slider": "Current CPU usage %",
          "id": "cpu_usage",
          "min": 0,
          "max": 100,
          "default": 50
        },
        {
          "slider": "Current memory usage %",
          "id": "memory_usage",
          "min": 0,
          "max": 100,
          "default": 70
        },
        {
          "text": "CRITICAL: Both CPU and memory maxed out. System is in death spiral.",
          "id": "death_spiral",
          "when": "cpu_usage > 90 && memory_usage > 90"
        },
        {
          "id": "emergency_action",
          "options": [
            "Restart application servers immediately",
            "Increase instance capacity",
            "Enable emergency rate limiting",
            "Activate circuit breakers"
          ],
          "when": "cpu_usage > 90 && memory_usage > 90",
          "select": "Emergency action required:"
        }
      ]
    },
    {
      "group": "Hypothesis Ranking",
      "id": "hypothesis_ranking",
      "elements": [
        {
          "text": "Based on your answers, adjust these hypothesis probabilities:",
          "id": "hypothesis_intro"
        },
        {
          "slider": "Database query problem",
          "id": "db_hypothesis",
          "min": 0,
          "max": 100,
          "default": 60
        },
        {
          "slider": "Memory/GC issue",
          "id": "memory_hypothesis",
          "min": 0,
          "max": 100,
          "default": 30
        },
        {
          "slider": "External service degradation",
          "id": "external_hypothesis",
          "min": 0,
          "max": 100,
          "default": 20
        },
        {
          "text": "High confidence in database issue. Priority actions:",
          "id": "db_priority",
          "when": "db_hypothesis > 80"
        },
        {
          "id": "db_diagnostics",
          "options": [
            "SHOW PROCESSLIST / pg_stat_activity",
            "Check for table locks",
            "Identify slow queries from logs",
            "Analyze query execution plans",
            "Check index usage statistics"
          ],
          "when": "db_hypothesis > 80",
          "multi": "Execute these database diagnostics immediately:"
        }
      ]
    },
    {
      "group": "Verification Loop",
      "id": "verification_loop",
      "elements": [
        {
          "text": "After taking initial actions, let's verify impact:",
          "id": "verification_intro"
        },
        {
          "slider": "Response time now (ms)",
          "id": "response_time_now",
          "min": 200,
          "max": 8000,
          "default": 4000
        },
        {
          "text": "Still experiencing slowness. This suggests:",
          "id": "still_slow",
          "when": "response_time_now > 1000"
        },
        {
          "id": "revised_hypothesis",
          "options": [
            "We fixed a symptom, not the root cause",
            "Multiple problems exist simultaneously",
            "The issue is propagating from upstream",
            "Client retries are amplifying the problem"
          ],
          "when": "response_time_now > 1000",
          "select": "Revised hypothesis:"
        },
        {
          "text": "RETRY STORM DETECTED: Each timeout triggers more requests, creating a feedback loop.",
          "id": "retry_storm",
          "when": "selected(revised_hypothesis, 'Client retries are amplifying the problem')"
        },
        {
          "id": "enable_backpressure",
          "default": true,
          "when": "selected(revised_hypothesis, 'Client retries are amplifying the problem')",
          "check": "Enable backpressure/rate limiting immediately?"
        },
        {
          "text": "Significant improvement! The intervention is working.",
          "id": "improvement",
          "when": "response_time_now <= 500"
        },
        {
          "id": "document_postmortem",
          "default": true,
          "when": "response_time_now <= 500",
          "check": "Document the root cause for post-mortem?"
        }
      ]
    },
    {
      "text": "Low DB probability + High external service probability = Check third-party dependencies",
      "id": "external_dependency_check",
      "when": "db_hypothesis < 30 && external_hypothesis > 50"
    },
    {
      "id": "external_services",
      "options": [
        "Payment gateway",
        "Email service",
        "CDN/Static assets",
        "Authentication provider",
        "Analytics/Monitoring",
        "Search service"
      ],
      "when": "db_hypothesis < 30 && external_hypothesis > 50",
      "multi": "Which external services should we check?"
    }
  ]
}